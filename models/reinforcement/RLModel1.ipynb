{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RLModel1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C176hV2VDyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a644b5fb-51d1-435b-b5a1-1063dc6d44f9"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#make the environment as Frozen Lake\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "#get the action space size\n",
        "action_space_size = env.action_space.n  #no. of columns of the Q table\n",
        "#get the state space size\n",
        "state_space_size = env.observation_space.n #no. of rows of the Q table\n",
        "\n",
        "#Build the q (state,action value function) table with no. of rows as states and no.of columns as actions. This is initialized to zero\n",
        "q_table = np.zeros((state_space_size,action_space_size))\n",
        "print(q_table)\n",
        "print(f\"There are {state_space_size} states and {action_space_size} actions\")\n",
        "\n",
        "\n",
        "#####################################  TRAINING #######################################################\n",
        "#set the number of episodes\n",
        "num_episodes = 10000\n",
        "#set the maximum number of steps per episode. Episode will terminate once reached this even if other episode termination conditions are not reached.\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "#set the learning rate\n",
        "learning_rate = 0.1\n",
        "#set the expected return discount rate\n",
        "discount_rate = 0.99\n",
        "\n",
        "#set the exploration rate \n",
        "exploration_rate = 1\n",
        "#set the maximum exploration rate\n",
        "max_exploration_rate = 1\n",
        "#set the minimum exploratino rate\n",
        "min_exploration_rate = 0.01\n",
        "#set the exploration rate decay (exploration reduction rate)\n",
        "exploration_decay_rate = 0.002\n",
        "\n",
        "#create an empty list to store rewards. This is the list containing total rewards at each episode\n",
        "rewards_all_episodes = []\n",
        "\n",
        "#Now, implement the Q-Learning Algorithm\n",
        "for episode in range(num_episodes):\n",
        "  #reset the state to initial state each time a new episode begins\n",
        "  state = env.reset()\n",
        "  #done stores the end of episode. \n",
        "  done = False\n",
        "  #track the rewards for the current episode\n",
        "  rewards_current_episode = 0\n",
        "\n",
        "  #run through each time step of the episode.\n",
        "  for time_steps in range(max_steps_per_episode):\n",
        "    #Now, it will be decided whether to select exploration or exploitation based on a random value generated\n",
        "    exploration_rate_threshold = random.uniform(0,1)\n",
        "\n",
        "    #select the action by exploitation of Q-table if the random rate is greater than exploration rate\n",
        "    if exploration_rate_threshold > exploration_rate:\n",
        "      action = np.argmax(q_table[state,:])\n",
        "    #otherwise, explore the environment by just selecting a random action from the action space\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "    #Execute the action in the environment to return a tuple containing information about coming into a new state, get the reward, done (episode over/not) and diagnostic information about envrionment\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    #Update the Q-table element for the corresponding state-action pair with equation q(s,a) = (1-lr)*(q(s,a)) + (lr)*(reward + gamma*(action value maximizing reward for the next state))\n",
        "    q_table[state,action] = q_table[state,action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state,:]))\n",
        "\n",
        "    #set the state for next time step\n",
        "    state = new_state\n",
        "    #add the previous state reward to the total rewards for the episode.\n",
        "    rewards_current_episode += reward\n",
        "\n",
        "    #stop going to the next time step if the current episode is done.\n",
        "    if done == True:\n",
        "      break\n",
        "  \n",
        "  #update the exploration rate at the end of each episode based on the exploration rate decay.\n",
        "  exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)*np.exp(-exploration_decay_rate*episode)\n",
        "\n",
        "  #append the current episode reward to all episode rewards\n",
        "  rewards_all_episodes.append(rewards_current_episode)\n",
        "\n",
        "#print the updated Q_table\n",
        "print(q_table)\n",
        "\n",
        "#print average reward per set\n",
        "sets = 20\n",
        "reward_per_set = np.split(np.array(rewards_all_episodes),sets) #divide the list containing all rewards into sub-arrays\n",
        "\n",
        "set_size = float(num_episodes/sets)\n",
        "for i in range(sets):\n",
        "  print(\"average reward\", sum(reward_per_set[i])/set_size,\"\\n\")\n",
        "\n",
        "\n",
        "#####################################  TESTING #######################################################\n",
        "\n",
        "test_episodes = 2\n",
        "#Now run the game for set of episodes\n",
        "for episode in range(test_episodes):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  print(\"Episode: \",episode+1,\"\\n\")\n",
        "  #wait for 2 s\n",
        "  time.sleep(2)\n",
        "\n",
        "  for steps in range(max_steps_per_episode):\n",
        "    #clears the screen; but wait until next clear output is called\n",
        "    clear_output(wait=True)\n",
        "    #display a pop-up window showing the showing the environment, agent on the sreen\n",
        "    env.render()\n",
        "    time.sleep(0.3)\n",
        "\n",
        "    action = np.argmax(q_table[state,:])\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      clear_output(wait=True)\n",
        "      env.render()\n",
        "      if reward == 1:\n",
        "        print(\"you reached the goal\",\"\\n\")\n",
        "        time.sleep(3)\n",
        "      else:\n",
        "        print(\"you fell through a hole\")\n",
        "        time.sleep(3)\n",
        "      clear_output(wait=True)\n",
        "      break\n",
        "    state = new_state\n",
        "#close the environment at last\n",
        "env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "you fell through a hole\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDdAlaUhuxoH",
        "outputId": "30ad4eb1-713c-45b8-8aa7-cde149a82eea"
      },
      "source": [
        "x = np.array([y for y in range(100)])\n",
        "x = np.split(x,5)\n",
        "print(str(sum(x[0])/20))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.5\n"
          ]
        }
      ]
    }
  ]
}