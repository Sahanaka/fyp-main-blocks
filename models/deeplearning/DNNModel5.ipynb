{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model 5 - RNN_LSTM for image feature output of model3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM+eBiaPOqCiy6LXvID40Fo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fFW-vy_8Yp2W"},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.utils import plot_model\n","from sklearn.model_selection import train_test_split\n","#from sklearn.compose import make_column_transformer\n","from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n","#from sklearn.datasets import make_circles\n","from sklearn.utils import shuffle\n","import sklearn\n","import itertools\n","import random\n","from tensorflow.keras.datasets import imdb\n","#from keras.preprocessing import image\n","from keras.preprocessing import sequence\n","import os\n","from google.colab import files\n","#from keras.preprocessing.image import ImageDataGenerator\n","\n","def normalizeMinMax(x):\n","  minimum = tf.math.reduce_min(x,axis=0)\n","  maximum = tf.math.reduce_max(x,axis=0)\n","  return (x - minimum)/(maximum - minimum)\n","\n","def plotting_model(train_data=[],train_labels=[],test_data=[],test_labels=[],prediction1=[],prediction2=[],prediction3=[]):\n","    plt.figure(figsize=(10,7))\n","    plt.scatter(train_data, train_labels, c=\"b\", label=\"Training_data\")\n","    plt.scatter(test_data, test_labels, c='r', label=\"Testing_data\")\n","    plt.scatter(test_data, prediction1, c='g', label=\"Predicted_data_model1\")\n","    plt.scatter(test_data, prediction2, c='y', label=\"Predicted_data_model2\")\n","    plt.scatter(test_data, prediction3, c='c', label=\"Predicted_data_model3\")\n","\n","def plot_decision_boundary(model,X,y):\n","  \"\"\"\n","  plots the decision boundary for a model classifying features (X) into classes\n","  \"\"\"\n","  #define the axis boundaries of the meshgrid\n","  x_min,x_max = X[:,0].min() - 0.1, X[:,0].max() + 0.1\n","  y_min,y_max = X[:,1].min() - 0.1, X[:,1].max() + 0.1\n","  xx, yy = np.meshgrid(np.linspace(x_min,x_max,100),np.linspace(y_min,y_max,100))\n","  \n","  #create x value\n","  x_in = np.c_[xx.ravel(),yy.ravel()]\n","  y_pred = model.predict(x_in)\n","  if y_pred[0] > 1:\n","    print(\"doing multiclass classfication\")\n","    y_pred = np.argmax(y_pred,axis=1).reshape(xx.shape)\n","  else:\n","    print(\"doing binary classification\")\n","    y_pred = np.round(y_pred).reshape(xx.shape)\n","  #plot the decision boundary\n","  plt.contourf(xx,yy,y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n","  plt.scatter(X[:,0],X[:,1],c='r',s=40,cmap=plt.cm.RdYlBu)\n","  plt.xlim(xx.min(),xx.max())\n","  plt.ylim(yy.min(),yy.max())\n","\n","def pretty_confusion_matrix(y_test,y_pred,classes=None,textsize=20):\n","  \n","  cm = sklearn.metrics.confusion_matrix(y_test,tf.round(y_pred))\n","  cm_norm = cm.astype(\"float\")/cm.sum(axis=1)[:,np.newaxis] #normalize the confusion matrix\n","  n_classes = cm.shape[0] #number of classes\n","\n","  #prettifying it.\n","  fig, ax = plt.subplots(figsize = (25,25))\n","  #create a matrix plot\n","  cax = ax.matshow(cm,cmap=plt.cm.Blues)\n","  fig.colorbar(cax)\n","\n","  #create classes\n","\n","  if classes:\n","    labels = classes\n","  else:\n","    labels = np.arange(cm.shape[0])\n","  \n","  #Label the axes\n","  ax.set(title=\"confusion matrix\",xlabel=\"predicted label\",ylabel=\"true label\",\n","         xticks=np.arange(n_classes),yticks=np.arange(n_classes),\n","         xticklabels = labels, yticklabels=labels)\n","  \n","  #set x-axis labels to bottom\n","  ax.xaxis.set_label_position(\"bottom\")\n","  ax.xaxis.tick_bottom()\n","\n","  #Adjust label \n","  ax.yaxis.label.set_size(textsize)\n","  ax.xaxis.label.set_size(textsize)\n","  ax.title.set_size(textsize)\n","\n","  #set threshold for different colors\n","  threshold = (cm.max() + cm.min())/2.0\n","\n","  #plot text on each cell\n","  for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n","    plt.text(j,i,f\"{cm[i,j]} ({cm_norm[i,j]*100:.1f}%)\",\n","             horizontalalignment=\"center\",\n","             color=\"white\" if cm[i,j]>threshold else \"black\", size=15)\n","\n","def plot_random_data(train_data,train_labels,class_names):\n","  #plt.figure(figsize=(10,10))\n","  for i in range(1):\n","    #ax = plt.subplot(1,1,i+1)\n","    index = random.choice(range(100))\n","    inner_index = int(train_labels[index])\n","    #plt.imshow(train_data[index],cmap=plt.cm.binary)\n","    print(\"true class: \",class_names[inner_index])\n","    #plt.axis(False)\n","\n","def test_random_data(test_data,test_labels,class_names,y_pred1):\n","    #plt.figure(figsize=(15,15))\n","    index = random.choice(range(len(test_data)))\n","    #plt.imshow(test_data[index],cmap=plt.cm.binary)\n","    inner_index = [int(test_labels[index]),int(test_labels[index+1]),int(test_labels[index+2]),int(test_labels[index+3]),int(test_labels[index+4]),int(test_labels[index+5]),int(test_labels[index+6]),int(test_labels[index+7]),int(test_labels[index+8]),int(test_labels[index+9])]\n","    print(\"true classes: \",class_names[inner_index[0]],class_names[inner_index[1]],class_names[inner_index[2]],class_names[inner_index[3]],class_names[inner_index[4]],class_names[inner_index[5]],class_names[inner_index[6]],class_names[inner_index[7]],class_names[inner_index[8]],class_names[inner_index[9]])\n","    #print(\" predicted classes: \",class_names[tf.argmax(y_pred1[inner_index[0]]).numpy()],class_names[tf.argmax(y_pred1[inner_index[1]]).numpy()],class_names[tf.argmax(y_pred1[inner_index[2]]).numpy()],class_names[tf.argmax(y_pred1[inner_index[3]]).numpy()],class_names[tf.argmax(y_pred1[inner_index[4]]).numpy()],class_names[tf.argmax(y_pred1[inner_index[5]]).numpy()],class_names[tf.argmax(y_pred1[inner_index[6]]).numpy()],class_names[tf.argmax(y_pred1[inner_index[7]]).numpy()],class_names[tf.argmax(y_pred1[inner_index[8]]).numpy()],class_names[tf.argmax(y_pred1[inner_index[9]]).numpy()])\n","    print(\" predicted classes: \",class_names[y_pred1[inner_index[0]]],class_names[y_pred1[inner_index[1]]],class_names[y_pred1[inner_index[2]]],class_names[y_pred1[inner_index[3]]],class_names[y_pred1[inner_index[4]]],class_names[y_pred1[inner_index[5]]],class_names[y_pred1[inner_index[6]]],class_names[y_pred1[inner_index[7]]],class_names[y_pred1[inner_index[8]]],class_names[y_pred1[inner_index[9]]])\n","    #plt.axis(False)\n","\n","def image_augmentation(train_data,train_labels,width,height,channels,size):\n","  #creates a data generator object that transforms the image\n","  datagen = ImageDataGenerator(rotation_range=40,width_shift_range=0.2,height_shift_range=0.2,\n","                                shear_range=0.2,zoom_range=0.2,horizontal_flip=True,fill_mode='nearest')\n","  #pick a random image to transform\n","  for j in range(size):\n","    index = random.choice(range(len(train_data)))\n","    test_im = train_data[index]\n","    img = image.img_to_array(test_im)\n","    img = img.reshape((1,)+img.shape)\n","    \n","    i=0\n","    for batch in datagen.flow(img,save_prefix=\"test\",save_format=\"jpeg\"):\n","      #plt.figure(i)\n","      #plot = plt.imshow(tf.cast(image.img_to_array(batch[0]),dtype=tf.int32))\n","      print(j,\" \", i,\"\\n\")\n","      casted_img = tf.cast(image.img_to_array(batch[0]),dtype=tf.int32)\n","      casted_img = tf.reshape(casted_img,shape=[1,width,height,channels])\n","      train_data = tf.concat(values=[train_data,casted_img],axis=0)\n","      reshaped_label = tf.reshape(train_labels[index],shape=[1,1])\n","      train_labels = tf.concat(values=[train_labels,reshaped_label],axis=0)\n","      i += 1\n","      if i > 4:\n","        break\n","      #plt.show()\n","  return train_data,train_labels\n","\n","def test_manual(text,model,max,class_names):\n","  word_index = imdb.get_word_index()#retrieve the imdb dictionary\n","  #print(word_index)\n","  tokens = tf.keras.preprocessing.text.text_to_word_sequence(input_text=text,filters='!$%().*+-/<=>?@[\\\\]^_{|}~\\t\\n',split=' ')#convert the input text into a sequence\n","  #print(tokens)\n","  new_token = [word_index[word] if word in word_index else 0 for word in tokens] #list comprehension of sequence to encode numerically according to imdb dictionary\n","  #print(new_token)\n","  padded_seq = sequence.pad_sequences(sequences=[new_token],maxlen=max)\n","  y_pred1 = model.predict(padded_seq)\n","  y_pred1 = tf.squeeze(tf.cast(tf.math.round(y_pred1),dtype=tf.int32))#for binary classification models\n","  print(\" predicted classes: \",class_names[y_pred1])\n","  return padded_seq[0]\n","\n","def decode_text(padded_seq):\n","  word_index = imdb.get_word_index()#retrieve the imdb dictionary\n","  reverse_word_index = {value:key for (key,value) in word_index.items()}#reverse the imdb dictionary\n","  #print(reverse_word_index)\n","  PAD = 0\n","  text = \"\"\n","  #print(padded_seq.shape)\n","  for x in padded_seq: \n","    if x != PAD:\n","      text += reverse_word_index[x] +  \" \"\n","  print(text[:-1])\n","\n","def split_input(text1):\n","  input_text = text1[:-1]\n","  output_text = text1[1:]\n","  return input_text,output_text\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":901},"id":"8-UdnQWpZA5L","executionInfo":{"status":"ok","timestamp":1622484098097,"user_tz":-330,"elapsed":1162,"user":{"displayName":"P.A.D. Shehan Nilmantha Wijesekara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8RfvZXKz84CXkRJ8-GnI8w2SRVGC02UWJlYMC=s64","userId":"02692999607457084522"}},"outputId":"b462b338-517b-4142-8322-9665ca9788f2"},"source":["#step 0: setting up (Preprocessing - Turn data into tensors) the data\n","\n","path_to_file = tf.keras.utils.get_file('shakespear.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n","#read and decode to python readable format\n","text = open(path_to_file,'rb').read().decode(encoding='utf-8')\n","print(len(text))\n","print(text[:250])\n","\n","#create a sorted list of characters in ascending order\n","vocabulary = sorted(set(text)) #convert the full text into a character set (sequence) and then arrange in ascending order\n","#enumerate the character set with integers as a nested list with tuples.\n","vocab_enum = list(enumerate(vocabulary))\n","#create a dictionary with integer as index\n","int2char = {x:y for x,y in vocab_enum}\n","print(int2char.items())\n","#create a dictionary with character as index\n","char2int = {y:x for x,y in int2char.items()}\n","print(char2int.items())\n","\n","def text_to_int(text):\n","  return np.array([char2int[c] for c in text])\n","\n","print(text_to_int(\"I love you\"))\n","\n","def int_to_text(ints):\n","  try:\n","    ints = ints.numpy()\n","  except:\n","    pass\n","  finally:\n","    tuple1 = (int2char[x] for x in ints)\n","  return \"\".join(tuple1)\n","\n","print(int_to_text(text_to_int(\"I love you\")))\n","\n","print(len(vocab_enum))\n","VOCAB_SIZE = len(vocabulary)  #number of vocabulary words.\n","SEQ_LEN = 100      #Maximum sequence length.\n","\n","\n","#Encode our whole dataset \n","dataset = tf.data.Dataset.from_tensor_slices(text_to_int(text))\n","\n","#divide the dataset into batches of sequence length\n","sequences = dataset.batch(SEQ_LEN+1,drop_remainder=True)\n","combined_data = sequences.map(split_input)\n","\n","BATCH_SIZE =64\n","BUFFER_SIZE = 10000\n","\n","for x,y in combined_data.take(1):\n","  print(\"INPUT: \",int_to_text(x),\"\\n\")\n","  print(\"OUTPUt: \",int_to_text(y),\"\\n\")\n","\n","shuffled_combined_data = (combined_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n","\n","print(shuffled_combined_data)\n","\"\"\"\n","(train_data,train_labels),(test_data,test_labels) = imdb.load_data(num_words=VOCAB_SIZE)\n","\n","\n","#resize the sequence to common length. If greater than max size; trim. otherwise, pad.\n","resized_train_data = tf.cast(sequence.pad_sequences(sequences=train_data,maxlen=MAX_LEN,padding='pre',truncating='pre'),dtype=tf.int32)\n","resized_test_data = tf.cast(sequence.pad_sequences(sequences=test_data,maxlen=MAX_LEN,padding='pre',truncating='pre'),dtype=tf.int32)\n","print(len(resized_train_data[0]))\n","print(len(resized_test_data[0]))\n","\n","\"\"\"\n","\"\"\"\n","#augmentation of images and addition to the training sets\n","augmented_train_data, augmented_train_labels = image_augmentation(resized_train_data,train_labels,width=width,height=height,channels=channels,size=500)\n","#print(train_data[0].shape,train_labels[0].shape)\n","#print(len(augmented_train_data),len(augmented_train_labels))\n","\n","\n","#shuffle the training data and labels\n","shuffled_aug_train_data, shuffled_aug_train_labels = shuffle(np.array(augmented_train_data),np.array(augmented_train_labels))\n","print(shuffled_aug_train_data[1],shuffled_aug_train_labels[1])\n","print(len(shuffled_aug_train_data),len(shuffled_aug_train_labels))\n","\n","\"\"\"\n","\"\"\"\n","#convert class names into human readable format\n","class_names = [\"Negative\",\"Positive\"]\n","plot_random_data(train_data,train_labels,class_names)\n","\n","\n","X_train,y_train,X_test,y_test = resized_train_data,train_labels,resized_test_data,test_labels\n","\n","#Trick-Normalize data; but don't normalize the labels:if possible one-hot encode the labels\n","#X_train,y_train,X_test,y_test = normalizeMinMax(resized_train_data),train_labels,normalizeMinMax(resized_test_data),test_labels\n","print(len(X_train),len(y_train))\n","print(X_train[0])\n","print(y_train[0:25])\n","\"\"\"\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1122304/1115394 [==============================] - 0s 0us/step\n","1115394\n","First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","dict_items([(0, '\\n'), (1, ' '), (2, '!'), (3, '$'), (4, '&'), (5, \"'\"), (6, ','), (7, '-'), (8, '.'), (9, '3'), (10, ':'), (11, ';'), (12, '?'), (13, 'A'), (14, 'B'), (15, 'C'), (16, 'D'), (17, 'E'), (18, 'F'), (19, 'G'), (20, 'H'), (21, 'I'), (22, 'J'), (23, 'K'), (24, 'L'), (25, 'M'), (26, 'N'), (27, 'O'), (28, 'P'), (29, 'Q'), (30, 'R'), (31, 'S'), (32, 'T'), (33, 'U'), (34, 'V'), (35, 'W'), (36, 'X'), (37, 'Y'), (38, 'Z'), (39, 'a'), (40, 'b'), (41, 'c'), (42, 'd'), (43, 'e'), (44, 'f'), (45, 'g'), (46, 'h'), (47, 'i'), (48, 'j'), (49, 'k'), (50, 'l'), (51, 'm'), (52, 'n'), (53, 'o'), (54, 'p'), (55, 'q'), (56, 'r'), (57, 's'), (58, 't'), (59, 'u'), (60, 'v'), (61, 'w'), (62, 'x'), (63, 'y'), (64, 'z')])\n","dict_items([('\\n', 0), (' ', 1), ('!', 2), ('$', 3), ('&', 4), (\"'\", 5), (',', 6), ('-', 7), ('.', 8), ('3', 9), (':', 10), (';', 11), ('?', 12), ('A', 13), ('B', 14), ('C', 15), ('D', 16), ('E', 17), ('F', 18), ('G', 19), ('H', 20), ('I', 21), ('J', 22), ('K', 23), ('L', 24), ('M', 25), ('N', 26), ('O', 27), ('P', 28), ('Q', 29), ('R', 30), ('S', 31), ('T', 32), ('U', 33), ('V', 34), ('W', 35), ('X', 36), ('Y', 37), ('Z', 38), ('a', 39), ('b', 40), ('c', 41), ('d', 42), ('e', 43), ('f', 44), ('g', 45), ('h', 46), ('i', 47), ('j', 48), ('k', 49), ('l', 50), ('m', 51), ('n', 52), ('o', 53), ('p', 54), ('q', 55), ('r', 56), ('s', 57), ('t', 58), ('u', 59), ('v', 60), ('w', 61), ('x', 62), ('y', 63), ('z', 64)])\n","[21  1 50 53 60 43  1 63 53 59]\n","I love you\n","65\n","INPUT:  First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You \n","\n","OUTPUt:  irst Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You  \n","\n","<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n#convert class names into human readable format\\nclass_names = [\"Negative\",\"Positive\"]\\nplot_random_data(train_data,train_labels,class_names)\\n\\n\\nX_train,y_train,X_test,y_test = resized_train_data,train_labels,resized_test_data,test_labels\\n\\n#Trick-Normalize data; but don\\'t normalize the labels:if possible one-hot encode the labels\\n#X_train,y_train,X_test,y_test = normalizeMinMax(resized_train_data),train_labels,normalizeMinMax(resized_test_data),test_labels\\nprint(len(X_train),len(y_train))\\nprint(X_train[0])\\nprint(y_train[0:25])\\n'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"id":"RTdo3nIIZGJK","executionInfo":{"status":"ok","timestamp":1622484112666,"user_tz":-330,"elapsed":1003,"user":{"displayName":"P.A.D. Shehan Nilmantha Wijesekara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8RfvZXKz84CXkRJ8-GnI8w2SRVGC02UWJlYMC=s64","userId":"02692999607457084522"}},"outputId":"88856977-1d78-453b-8be4-5a23f9233afa"},"source":["#step 1: Creating the models- Define input layer, hidden layers and output layer\n","RNN_UNITS = 1024\n","EMBEDDING_DIM = 256\n","\n","tf.random.set_seed(40)\n","model1 = tf.keras.Sequential(name='RNN1')\n","model1.add(tf.keras.layers.Embedding(input_dim = VOCAB_SIZE,output_dim=EMBEDDING_DIM,batch_input_shape=[BATCH_SIZE,None],name=\"Embedding_layer\"))#input_dim: maximum size of the vocabulary, this convert positive integer indexes to a vector\n","model1.add(tf.keras.layers.LSTM(units=RNN_UNITS,return_sequences=True,stateful=True,activation='tanh',recurrent_initializer='glorot_uniform',recurrent_activation='sigmoid'))#LSTM layer which return sequences:return the last output for each time step, last state of a batch will be used as the initial state of the next batch\n","model1.add(tf.keras.layers.Dense(units=256,activation='relu',name='hidden_dense_layer1'))\n","model1.add(tf.keras.layers.Dense(units=VOCAB_SIZE,name='output_layer'))\n","\"\"\"\n","model2 = tf.keras.Sequential(name='neural_network2')\n","model2.add(tf.keras.layers.Dense(100, activation='relu',name='Hidden_layer1'))  # Hidden layer 1 with 100 neurons\n","model2.add(tf.keras.layers.Dense(100, activation='relu', name='Hidden_layer2'))  # Hidden layer 2 with 100 neurons\n","model2.add(tf.keras.layers.Dense(1, name='output_layer'))  # output layer\n","\n","model3 = tf.keras.Sequential(name='neural_network3')\n","model3.add(tf.keras.layers.Dense(50,activation='relu',name='Hidden_layer1')) #Hidden layer 1 with 10 neurons\n","model3.add(tf.keras.layers.Dense(100,activation='relu',name='Hidden_layer2')) #Hidden layer 2 with 100 neurons\n","model3.add(tf.keras.layers.Dense(500,activation='relu',name='Hidden_layer3')) #Hidden layer 3 with 500 neurons\n","model3.add(tf.keras.layers.Dense(100, activation='relu', name='Hidden_layer4'))#Hidden layer 4 with 100 neurons\n","model3.add(tf.keras.layers.Dense(10, activation='relu', name='Hidden_layer5'))#Hidden layer 5 with 10 neurons\n","model3.add(tf.keras.layers.Dense(1,name='output_layer')) #output layer\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nmodel2 = tf.keras.Sequential(name='neural_network2')\\nmodel2.add(tf.keras.layers.Dense(100, activation='relu',name='Hidden_layer1'))  # Hidden layer 1 with 100 neurons\\nmodel2.add(tf.keras.layers.Dense(100, activation='relu', name='Hidden_layer2'))  # Hidden layer 2 with 100 neurons\\nmodel2.add(tf.keras.layers.Dense(1, name='output_layer'))  # output layer\\n\\nmodel3 = tf.keras.Sequential(name='neural_network3')\\nmodel3.add(tf.keras.layers.Dense(50,activation='relu',name='Hidden_layer1')) #Hidden layer 1 with 10 neurons\\nmodel3.add(tf.keras.layers.Dense(100,activation='relu',name='Hidden_layer2')) #Hidden layer 2 with 100 neurons\\nmodel3.add(tf.keras.layers.Dense(500,activation='relu',name='Hidden_layer3')) #Hidden layer 3 with 500 neurons\\nmodel3.add(tf.keras.layers.Dense(100, activation='relu', name='Hidden_layer4'))#Hidden layer 4 with 100 neurons\\nmodel3.add(tf.keras.layers.Dense(10, activation='relu', name='Hidden_layer5'))#Hidden layer 5 with 10 neurons\\nmodel3.add(tf.keras.layers.Dense(1,name='output_layer')) #output layer\\n\""]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DZUDiVT8et1k","executionInfo":{"status":"ok","timestamp":1622484122109,"user_tz":-330,"elapsed":5504,"user":{"displayName":"P.A.D. Shehan Nilmantha Wijesekara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8RfvZXKz84CXkRJ8-GnI8w2SRVGC02UWJlYMC=s64","userId":"02692999607457084522"}},"outputId":"a3063195-6cc6-4ebb-f10b-60f6310097b6"},"source":["#try the model\n","for input_example_batch, target_example_batch in shuffled_combined_data.take(1):\n","    example_batch_predictions = model1(input_example_batch)\n","    print(input_example_batch.shape)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","\n","model1.summary()\n","print(example_batch_predictions[0])\n","print(\"sequence_length,vocab_size\",example_batch_predictions[0].shape)\n","\n","#take one random sample out of 65 logits (A logit is an input to a softmax function)\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","print(sampled_indices)\n","print(\"sequence_length\",sampled_indices.shape) \n","#remove the extra size 1 dimension\n","sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n","print(sampled_indices)\n","print(\"squeezed sample_indices\",sampled_indices.shape) \n","\n","print(\"Input:\\n\", int_to_text(input_example_batch[0]))\n","print(\"Next Char Predictions:\\n\", int_to_text(sampled_indices))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(64, 100)\n","(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n","Model: \"RNN1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","Embedding_layer (Embedding)  (64, None, 256)           16640     \n","_________________________________________________________________\n","lstm (LSTM)                  (64, None, 1024)          5246976   \n","_________________________________________________________________\n","hidden_dense_layer1 (Dense)  (64, None, 256)           262400    \n","_________________________________________________________________\n","output_layer (Dense)         (64, None, 65)            16705     \n","=================================================================\n","Total params: 5,542,721\n","Trainable params: 5,542,721\n","Non-trainable params: 0\n","_________________________________________________________________\n","tf.Tensor(\n","[[-0.00015927 -0.00075921  0.0011484  ...  0.00407171  0.00325431\n","   0.00345966]\n"," [-0.0040196   0.00108111  0.00278038 ...  0.0050939   0.00048026\n","   0.00442758]\n"," [-0.00327653  0.00265795  0.0012358  ...  0.00160849  0.0038464\n","   0.00245185]\n"," ...\n"," [ 0.00365443  0.0036553   0.00402467 ...  0.00819353 -0.00239304\n","   0.00477431]\n"," [ 0.00293229  0.00389525  0.00633261 ...  0.01082895 -0.00496217\n","   0.0063461 ]\n"," [-0.00096921  0.00774527  0.00993024 ...  0.00731147 -0.00336825\n","   0.00723737]], shape=(100, 65), dtype=float32)\n","sequence_length,vocab_size (100, 65)\n","tf.Tensor(\n","[[50]\n"," [ 6]\n"," [35]\n"," [30]\n"," [ 9]\n"," [ 9]\n"," [34]\n"," [18]\n"," [19]\n"," [16]\n"," [11]\n"," [ 8]\n"," [11]\n"," [ 8]\n"," [34]\n"," [ 8]\n"," [48]\n"," [38]\n"," [ 9]\n"," [51]\n"," [42]\n"," [39]\n"," [13]\n"," [62]\n"," [24]\n"," [ 7]\n"," [11]\n"," [41]\n"," [10]\n"," [20]\n"," [26]\n"," [24]\n"," [19]\n"," [27]\n"," [ 0]\n"," [19]\n"," [62]\n"," [37]\n"," [ 2]\n"," [27]\n"," [37]\n"," [61]\n"," [18]\n"," [48]\n"," [64]\n"," [62]\n"," [11]\n"," [51]\n"," [15]\n"," [51]\n"," [ 3]\n"," [20]\n"," [29]\n"," [29]\n"," [ 9]\n"," [45]\n"," [59]\n"," [12]\n"," [39]\n"," [12]\n"," [53]\n"," [39]\n"," [39]\n"," [31]\n"," [15]\n"," [45]\n"," [29]\n"," [22]\n"," [19]\n"," [61]\n"," [39]\n"," [51]\n"," [25]\n"," [31]\n"," [40]\n"," [19]\n"," [54]\n"," [49]\n"," [52]\n"," [50]\n"," [20]\n"," [38]\n"," [14]\n"," [ 0]\n"," [42]\n"," [11]\n"," [49]\n"," [35]\n"," [36]\n"," [55]\n"," [ 8]\n"," [23]\n"," [57]\n"," [45]\n"," [21]\n"," [21]\n"," [24]\n"," [30]\n"," [19]\n"," [26]], shape=(100, 1), dtype=int64)\n","sequence_length (100, 1)\n","[50  6 35 30  9  9 34 18 19 16 11  8 11  8 34  8 48 38  9 51 42 39 13 62\n"," 24  7 11 41 10 20 26 24 19 27  0 19 62 37  2 27 37 61 18 48 64 62 11 51\n"," 15 51  3 20 29 29  9 45 59 12 39 12 53 39 39 31 15 45 29 22 19 61 39 51\n"," 25 31 40 19 54 49 52 50 20 38 14  0 42 11 49 35 36 55  8 23 57 45 21 21\n"," 24 30 19 26]\n","squeezed sample_indices (100,)\n","Input:\n"," fair-shining suns.\n","\n","RICHARD:\n","Nay, bear three daughters: by your leave I speak it,\n","You love the breed\n","Next Char Predictions:\n"," l,WR33VFGD;.;.V.jZ3mdaAxL-;c:HNLGO\n","GxY!OYwFjzx;mCm$HQQ3gu?a?oaaSCgQJGwamMSbGpknlHZB\n","d;kWXq.KsgIILRGN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6khM2aETZIp3"},"source":["#step 2: Compiling the  - Define the loss function and optimizer for NNs\n","model1.trainable = True\n","model1.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),metrics=['accuracy'])\n","#model2.compile(loss=tf.keras.losses.mae,optimizer=tf.keras.optimizers.Adam(),metrics=['mae'])\n","#model3.compile(loss=tf.keras.losses.mae, optimizer=tf.keras.optimizers.Adam(),metrics=['mae'])\n","\n","#step 3: Fitting (Tranining) the models\n","#A callback works during model training.  \n","Earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss',patience = 3)\n","learning_rate_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch:1e-3 * 10 **(epoch/7))#start with a low training rate and increase with number of epochs\n","#directory for checkpoint saving\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt_{epoch}\")\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)\n","\n","#training_curve1 = model1.fit(X_train,y_train,epochs=20,verbose=1,callbacks=[Earlystop_callback,learning_rate_scheduler_callback])\n","training_curve1 = model1.fit(shuffled_combined_data,epochs=1,verbose=1,callbacks=[Earlystop_callback,checkpoint_callback])\n","#training_curve2 = model2.fit(X_train_Normalized,y_train,epochs=500,verbose=0,callbacks=[Earlystop_callback,learning_rate_scheduler_callback])\n","#training_curve3 = model3.fit(X_train_Normalized, y_train,epochs=500,verbose=0callbacks=[Earlystop_callback,learning_rate_scheduler_callback])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"POgkndunZMHc"},"source":["#step 4: Evaluating the models\n","def generate_text(start_string):\n","  #number of characters to generate\n","  num_generate = 2\n","\n","  #converting start string to numbers\n","  input_eval = np.array([char2int[c] for c in start_string])\n","  print(input_eval,\"\\n\")\n","  #empty string to store results\n","  text_generated = \"\"\n","  temperature = 1.0\n","  BATCH_SIZE =64\n","  BUFFER_SIZE = 10000\n","\n","  #reset the model's states\n","  for i in range(num_generate):\n","    dataset = tf.data.Dataset.from_tensor_slices(input_eval)\n","    new_dataset = dataset.batch(SEQ_LEN+1,drop_remainder=True)\n","    latest_dataset = new_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n","    print(i)\n","    for input_example_batch in latest_dataset.take(1):\n","      predictions = model1(input_example_batch)\n","      print(input_example_batch.shape,\"\\n\")\n","      print(predictions.shape, \"# (batch_size, sequence_length, vocab_size)\",'\\n')\n","      print(i)\n","      \"\"\"\n","      #remove the batch dimension\n","      #predictions = tf.squeeze(predictions,axis=0)\n","\n","      #using a categorical distribution to predict the character returned by the model\n","      predictions = predictions/temperature\n","      predicted_id = tf.random.categorical(predictions[0],num_samples=1) #last character id is obtained\n","\n","      #pass this predicted character as the input to the next iteration with previous state\n","      input_eval = predicted_id\n","      text_generated.append(int2char[predicted_id])\n","      #print(predicted_id,\"\\n\")\n","\n","      \"\"\"\n","inp = \"Because narrative paragraphs resemble fiction an untrue story, you have a little more freedom to write the story in the style you prefer. This is known as artistic freedom or artistic license. You can use the first person narrative style and include words that clearly refer to you I, me, my, mine, etc., or you can try to tell the story from a purely objective point of view that is not personal but gives a straight-forward, factual account of what happened. If your teacher asks you to write about a personal experience, try to tell it by using the first person. This is the easiest style in which to write something. You might choose something that you remember well or something that changed your life. Teachers who work with a large number of foreign born students often ask them to write about the time they first arrived in the United States. That is a good assignment because it allows you to write in the first person and the details in this kind of paragraph are likely to be very vivid. Here is an example\"\n","\n","generate_text(start_string=inp)\n","\n","\n","\n","\n","\n","\"\"\"\n","model1.summary()\n","\n","#model2.summary()\n","#model3.summary()\n","y_pred1 = model1.predict(X_test)\n","y_pred1 = tf.squeeze(tf.cast(tf.math.round(y_pred1),dtype=tf.int32))#for binary classification models\n","#y_pred2 = model2.predict(X_test_Normalized)\n","#y_pred3 = model3.predict(X_test_Normalized)\n","model1.evaluate(X_test,y_test)\n","#model2.evaluate(X_test_Normalized,y_test)\n","#model3.evaluate(X_test_Normalized, y_test)\n","\n","#plotting_model(X_train, y_train, X_test, y_test, y_pred1, y_pred2, y_pred3)\n","plot_model(model=model1, show_shapes=True)\n","#plot_decision_boundary(model1,X_test,y_test)\n","test_random_data(test_data,test_labels,class_names,y_pred1)\n","#mae1 = tf.keras.metrics.mean_absolute_error(y_test,tf.argmax(y_pred1[0]).numpy())\n","#mse1 = tf.keras.metrics.mean_squared_error(y_test,tf.argmax(y_pred1[0]).numpy())\n","mae1 = tf.keras.metrics.mean_absolute_error(y_test,y_pred1[0])\n","mse1 = tf.keras.metrics.mean_squared_error(y_test,y_pred1[0])\n","print(mae1, mse1)\n","plt.figure(figsize=(10,7))\n","pd.DataFrame(training_curve1.history).plot()\n","plt.title(\"Training curve for model1\")\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"epochs\")\n","\n","\"\"\"\n","\"\"\"\n","#plot the learning rate vs the loss to find the ideal learning rate for a given model\n","lrs = 1e-4 * 10 **((tf.range(25))/20)\n","plt.figure(figsize=(10,7))\n","plt.semilogx(lrs,training_curve1.history[\"loss\"][:25])\n","plt.xlabel(\"learnig rate in log\")\n","plt.ylabel(\"loss\")\n","plt.title(\"loss vs learning rate\")\n","\"\"\"\n","\"\"\"\n","#classification_report1 = sklearn.metrics.classification_report(y_test,tf.argmax(y_pred1,axis=1).numpy())\n","#confusion_matrix1 = sklearn.metrics.confusion_matrix(y_test,tf.argmax(y_pred1,axis=1).numpy())\n","classification_report1 = sklearn.metrics.classification_report(y_test,y_pred1)\n","confusion_matrix1 = sklearn.metrics.confusion_matrix(y_test,y_pred1)\n","print(classification_report1)\n","print(confusion_matrix1)\n","#pretty_confusion_matrix(y_test,tf.argmax(y_pred1,axis=1).numpy(),classes=class_names)\n","pretty_confusion_matrix(y_test,y_pred1,classes=class_names)\n","\n","text1 = \"This is indeed a nice movie. Character are good. I really love it\"\n","test_manual(text=text1,model=model1,max=250,class_names=class_names)\n","\"\"\"\n","\"\"\"\n","#plot_model(model=model2, show_shapes=True)\n","mae2 = tf.keras.metrics.mean_absolute_error(y_test, tf.squeeze(y_pred2)).numpy()\n","mse2 = tf.keras.metrics.mean_squared_error(y_test, tf.squeeze(y_pred2)).numpy()\n","print(mae2, mse2)\n","pd.DataFrame(training_curve2.history).plot()\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"epochs\")\n","#plot_model(model=model3, show_shapes=True)\n","mae3 = tf.keras.metrics.mean_absolute_error(y_test, tf.squeeze(y_pred3)).numpy()\n","mse3 = tf.keras.metrics.mean_squared_error(y_test, tf.squeeze(y_pred3)).numpy()\n","print(mae3, mse3)\n","pd.DataFrame(training_curve3.history).plot()\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"epochs\")\n","\n","#step 5: Comparing the models\n","model_results = [[\"model1\",mae1,mse1],[\"model2\",mae2,mse2],[\"model3\",mae3,mse3]]\n","all_results = pd.DataFrame(model_results,columns=[\"Model_name\", \"MAE\",\"MSE\"])\n","print(all_results)\n","\n","#step 6: Saving the models\n","#Saving can be done in one of SavedModel format or in HDF5 format.\n","model1.save(\"model1Saved.h5\",save_format='HDF5')\n","model2.save(\"model2Saved\", save_format='SavedModel')\n","model3.save(\"model3Saved\", save_format='SavedModel')\n","\n","#loading model (optional)\n","loadmodel1 = tf.keras.models.load_model(\"model1Saved.h5\",custom_objects={\"mae\": tf.keras.losses.mae})\n","loadmodel1.summary()\n","loadmodel_pred = loadmodel1.predict(X_test)\n","original_pred = model1.predict(X_test)\n","print(loadmodel_pred == original_pred)\n","\"\"\"\n","\n"],"execution_count":null,"outputs":[]}]}